.TH HSPACE 1 2009-06-01 htools "Ganeti H-tools"
.SH NAME
hspace \- Cluster space analyzer for Ganeti

.SH SYNOPSIS
.B hspace
.B "[backend options...]"
.B "[algorithm options...]"
.B "[request options..."]
.BI "[ -p[" fields "] ]"
.B "[-v... | -q]"

.B hspace
.B --version

.TP
Backend options:
.BI " -m " cluster
|
.BI " -L[" path "]"
|
.BI " -t " data-file
|
.BI " --simulate " spec

.TP
Algorithm options:
.BI "[ --max-cpu " cpu-ratio " ]"
.BI "[ --min-disk " disk-ratio " ]"
.BI "[ -O " name... " ]"

.TP
Request options:
.BI "[--memory " mem "]"
.BI "[--disk " disk "]"
.BI "[--req-nodes " req-nodes "]"
.BI "[--vcpus " vcpus "]"
.BI "[--tiered-alloc " spec "]"


.SH DESCRIPTION
hspace computes how many additional instances can be fit on a cluster,
while maintaining N+1 status.

The program will try to place instances, all of the same size, on the
cluster, until the point where we don't have any N+1 possible
allocation. It uses the exact same allocation algorithm as the hail
iallocator plugin.

The output of the program is designed to interpreted as a shell
fragment (or parsed as a \fIkey=value\fR file). Options which extend
the output (e.g. \-p, \-v) will output the additional information on
stderr (such that the stdout is still parseable).

The following keys are available in the output of the script (all
prefixed with \fIHTS_\fR):
.TP
.I SPEC_MEM, SPEC_DSK, SPEC_CPU, SPEC_RQN
These represent the specifications of the instance model used for
allocation (the memory, disk, cpu, requested nodes).

.TP
.I CLUSTER_MEM, CLUSTER_DSK, CLUSTER_CPU, CLUSTER_NODES
These represent the total memory, disk, CPU count and total nodes in
the cluster.

.TP
.I INI_SCORE, FIN_SCORE
These are the initial (current) and final cluster score (see the hbal
man page for details about the scoring algorithm).

.TP
.I INI_INST_CNT, FIN_INST_CNT
The initial and final instance count.

.TP
.I INI_MEM_FREE, FIN_MEM_FREE
The initial and final total free memory in the cluster (but this
doesn't necessarily mean available for use).

.TP
.I INI_MEM_AVAIL, FIN_MEM_AVAIL
The initial and final total available memory for allocation in the
cluster. If allocating redundant instances, new instances could
increase the reserved memory so it doesn't necessarily mean the
entirety of this memory can be used for new instance allocations.

.TP
.I INI_MEM_RESVD, FIN_MEM_RESVD
The initial and final reserved memory (for redundancy/N+1 purposes).

.TP
.I INI_MEM_INST, FIN_MEM_INST
The initial and final memory used for instances (actual runtime used
RAM).

.TP
.I INI_MEM_OVERHEAD, FIN_MEM_OVERHEAD
The initial and final memory overhead \(em memory used for the node
itself and unacounted memory (e.g. due to hypervisor overhead).

.TP
.I INI_MEM_EFF, HTS_INI_MEM_EFF
The initial and final memory efficiency, represented as instance
memory divided by total memory.

.TP
.I INI_DSK_FREE, INI_DSK_AVAIL, INI_DSK_RESVD, INI_DSK_INST, INI_DSK_EFF
Initial disk stats, similar to the memory ones.

.TP
.I FIN_DSK_FREE, FIN_DSK_AVAIL, FIN_DSK_RESVD, FIN_DSK_INST, FIN_DSK_EFF
Final disk stats, similar to the memory ones.

.TP
.I INI_CPU_INST, FIN_CPU_INST
Initial and final number of virtual CPUs used by instances.

.TP
.I INI_CPU_EFF, FIN_CPU_EFF
The initial and final CPU efficiency, represented as the count of
virtual instance CPUs divided by the total physical CPU count.

.TP
.I INI_MNODE_MEM_AVAIL, FIN_MNODE_MEM_AVAIL
The initial and final maximum per\(hynode available memory. This is not
very useful as a metric but can give an impression of the status of
the nodes; as an example, this value restricts the maximum instance
size that can be still created on the cluster.

.TP
.I INI_MNODE_DSK_AVAIL, FIN_MNODE_DSK_AVAIL
Like the above but for disk.

.TP
.I TSPEC
If the tiered allocation mode has been enabled, this parameter holds
the pairs of specifications and counts of instances that can be
created in this mode. The value of the key is a space\(hyseparated list
of values; each value is of the form \fImemory,disk,vcpu=count\fR
where the memory, disk and vcpu are the values for the current spec,
and count is how many instances of this spec can be created. A
complete value for this variable could be: \fB4096,102400,2=225
2560,102400,2=20 512,102400,2=21\fR.

.TP
.I KM_USED_CPU, KM_USED_NPU, KM_USED_MEM, KM_USED_DSK
These represents the metrics of used resources at the start of the
computation (only for tiered allocation mode). The NPU value is
"normalized" CPU count, i.e. the number of virtual CPUs divided by the
maximum ratio of the virtual to physical CPUs.

.TP
.I KM_POOL_CPU, KM_POOL_NPU, KM_POOL_MEM, KM_POOL_DSK
These represents the total resources allocated during the tiered
allocation process. In effect, they represent how much is readily
available for allocation.

.TP
.I KM_UNAV_CPU, KM_POOL_NPU, KM_UNAV_MEM, KM_UNAV_DSK
These represents the resources left over (either free as in
unallocable or allocable on their own) after the tiered allocation has
been completed. They represent better the actual unallocable
resources, because some other resource has been exhausted. For
example, the cluster might still have 100GiB disk free, but with no
memory left for instances, we cannot allocate another instance, so in
effect the disk space is unallocable. Note that the CPUs here
represent instance virtual CPUs, and in case the \fI--max-cpu\fR
option hasn't been specified this will be \-1.

.TP
.I ALLOC_USAGE
The current usage represented as initial number of instances divided
per final number of instances.

.TP
.I ALLOC_COUNT
The number of instances allocated (delta between FIN_INST_CNT and
INI_INST_CNT).

.TP
.I ALLOC_FAIL*_CNT
For the last attemp at allocations (which would have increased
FIN_INST_CNT with one, if it had succeeded), this is the count of the
failure reasons per failure type; currently defined are FAILMEM,
FAILDISK and FAILCPU which represent errors due to not enough memory,
disk and CPUs, and FAILN1 which represents a non N+1 compliant cluster
on which we can't allocate instances at all.

.TP
.I ALLOC_FAIL_REASON
The reason for most of the failures, being one of the above FAIL*
strings.

.TP
.I OK
A marker representing the successful end of the computation, and
having value "1". If this key is not present in the output it means
that the computation failed and any values present should not be
relied upon.

.PP

If the tiered allocation mode is enabled, then many of the INI_/FIN_
metrics will be also displayed with a TRL_ prefix, and denote the
cluster status at the end of the tiered allocation run.

.SH OPTIONS
The options that can be passed to the program are as follows:

.TP
.BI "--memory " mem
The memory size of the instances to be placed (defaults to 4GiB).

.TP
.BI "--disk " disk
The disk size of the instances to be placed (defaults to 100GiB).

.TP
.BI "--req-nodes " num-nodes
The number of nodes for the instances; the default of two means
mirrored instances, while passing one means plain type instances.

.TP
.BI "--vcpus " vcpus
The number of VCPUs of the instances to be placed (defaults to 1).

.TP
.BI "--max-cpu " cpu-ratio
The maximum virtual\(hyto\(hyphysical cpu ratio, as a floating point
number between zero and one. For example, specifying \fIcpu-ratio\fR
as \fB2.5\fR means that, for a 4\(hycpu machine, a maximum of 10
virtual cpus should be allowed to be in use for primary instances. A
value of one doesn't make sense though, as that means no disk space
can be used on it.

.TP
.BI "--min-disk " disk-ratio
The minimum amount of free disk space remaining, as a floating point
number. For example, specifying \fIdisk-ratio\fR as \fB0.25\fR means
that at least one quarter of disk space should be left free on nodes.

.TP
.B -p, --print-nodes
Prints the before and after node status, in a format designed to allow
the user to understand the node's most important parameters.

It is possible to customise the listed information by passing a
comma\(hyseparated list of field names to this option (the field list
is currently undocumented), or to extend the default field list by
prefixing the additional field list with a plus sign. By default, the
node list will contain the following information:
.RS
.TP
.B F
a character denoting the status of the node, with '\-' meaning an
offline node, '*' meaning N+1 failure and blank meaning a good node
.TP
.B Name
the node name
.TP
.B t_mem
the total node memory
.TP
.B n_mem
the memory used by the node itself
.TP
.B i_mem
the memory used by instances
.TP
.B x_mem
amount memory which seems to be in use but cannot be determined why or
by which instance; usually this means that the hypervisor has some
overhead or that there are other reporting errors
.TP
.B f_mem
the free node memory
.TP
.B r_mem
the reserved node memory, which is the amount of free memory needed
for N+1 compliance
.TP
.B t_dsk
total disk
.TP
.B f_dsk
free disk
.TP
.B pcpu
the number of physical cpus on the node
.TP
.B vcpu
the number of virtual cpus allocated to primary instances
.TP
.B pcnt
number of primary instances
.TP
.B pcnt
number of secondary instances
.TP
.B p_fmem
percent of free memory
.TP
.B p_fdsk
percent of free disk
.TP
.B r_cpu
ratio of virtual to physical cpus
.TP
.B lCpu
the dynamic CPU load (if the information is available)
.TP
.B lMem
the dynamic memory load (if the information is available)
.TP
.B lDsk
the dynamic disk load (if the information is available)
.TP
.B lNet
the dynamic net load (if the information is available)
.RE

.TP
.BI "-O " name
This option (which can be given multiple times) will mark nodes as
being \fIoffline\fR, and instances won't be placed on these nodes.

Note that hspace will also mark as offline any nodes which are
reported by RAPI as such, or that have "?" in file\(hybased input in any
numeric fields.
.RE

.TP
.BI "-t" datafile ", --text-data=" datafile
The name of the file holding node and instance information (if not
collecting via RAPI or LUXI). This or one of the other backends must
be selected.

.TP
.BI "-S" filename ", --save-cluster=" filename
If given, the state of the cluster at the end of the allocation is
saved to a file named \fIfilename.alloc\fR, and if tiered allocation
is enabled, the state after tiered allocation will be saved to
\fIfilename.tiered\fR. This allows re-feeding the cluster state to
either hspace itself (with different parameters) or for example hbal.

.TP
.BI "-m" cluster
Collect data directly from the
.I cluster
given as an argument via RAPI. If the argument doesn't contain a colon
(:), then it is converted into a fully\(hybuilt URL via prepending
https:// and appending the default RAPI port, otherwise it's
considered a fully\(hyspecified URL and is used as\(hyis.

.TP
.BI "-L[" path "]"
Collect data directly from the master daemon, which is to be contacted
via the luxi (an internal Ganeti protocol). An optional \fIpath\fR
argument is interpreted as the path to the unix socket on which the
master daemon listens; otherwise, the default path used by ganeti when
installed with \fI--localstatedir=/var\fR is used.

.TP
.BI "--simulate " description
Instead of using actual data, build an empty cluster given a node
description. The \fIdescription\fR parameter must be a
comma\(hyseparated list of four elements, describing in order:

.RS

.RS
.TP
the number of nodes in the cluster

.TP
the disk size of the nodes, in mebibytes

.TP
the memory size of the nodes, in mebibytes

.TP
the cpu core count for the nodes

.RE

An example description would be \fB20,102400,16384,4\fR describing a
20\(hynode cluster where each node has 100GiB of disk space, 16GiB of
memory and 4 CPU cores. Note that all nodes must have the same specs
currently.

.RE

.TP
.BI "--tiered-alloc " spec
Beside the standard, fixed\(hysize allocation, also do a tiered
allocation scheme where the algorithm starts from the given
specification and allocates until there is no more space; then it
decreases the specification and tries the allocation again. The
decrease is done on the matric that last failed during allocation. The
specification given is similar to the \fI--simulate\fR option and it
holds:

.RS

.RS

.TP
the disk size of the instance

.TP
the memory size of the instance

.TP
the vcpu count for the insance

.RE

An example description would be \fB10240,8192,2\fR describing an
initial starting specification of 10GiB of disk space, 4GiB of memory
and 2 VCPUs.

Also note that the normal allocation and the tiered allocation are
independent, and both start from the initial cluster state; as such,
the instance count for these two modes are not related one to another.

.RE

.TP
.B -v, --verbose
Increase the output verbosity. Each usage of this option will increase
the verbosity (currently more than 2 doesn't make sense) from the
default of one. At verbosity 2 the location of the new instances is
shown in the standard error.

.TP
.B -q, --quiet
Decrease the output verbosity. Each usage of this option will decrease
the verbosity (less than zero doesn't make sense) from the default of
one.

.TP
.B -V, --version
Just show the program version and exit.

.SH EXIT STATUS

The exist status of the command will be zero, unless for some reason
the algorithm fatally failed (e.g. wrong node or instance data).

.SH BUGS

The algorithm is highly dependent on the number of nodes; its runtime
grows exponentially with this number, and as such is impractical for
really big clusters.

The algorithm doesn't rebalance the cluster or try to get the optimal
fit; it just allocates in the best place for the current step, without
taking into consideration the impact on future placements.

.SH ENVIRONMENT

If the variables \fBHTOOLS_NODES\fR and \fBHTOOLS_INSTANCES\fR are
present in the environment, they will override the default names for
the nodes and instances files. These will have of course no effect
when the RAPI or Luxi backends are used.

.SH SEE ALSO
.BR hbal "(1), " hscan "(1), " ganeti "(7), " gnt-instance "(8), "
.BR gnt-node "(8)"

.SH "COPYRIGHT"
.PP
Copyright (C) 2009 Google Inc. Permission is granted to copy,
distribute and/or modify under the terms of the GNU General Public
License as published by the Free Software Foundation; either version 2
of the License, or (at your option) any later version.
.PP
On Debian systems, the complete text of the GNU General Public License
can be found in /usr/share/common-licenses/GPL.
